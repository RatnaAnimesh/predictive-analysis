# Geopolitical Predictor

This project implements a system for geopolitical anomaly detection and aims to extend into predictive and corroboration analysis based on news data. It processes large volumes of GDELT news data to identify unusual patterns in entity mentions and sentiment.

## Table of Contents

- [Project Goals](#project-goals)
- [Setup](#setup)
- [Usage](#usage)
- [Data Sources](#data-sources)
- [Project Structure](#project-structure)

## Project Goals

1.  **Anomaly Detection:** Figure out if something is showing up in the news more to then check what's happening. Alerts if Z-score of occurrence is higher than 3.
2.  **Predictive Analysis:** Look at the latest news and try to figure out what *could* happen and assign an exact probability to the event. Should display major geopolitical or financial events occurring with probabilities more than 50%.
3.  **Corroboration Analysis:** Say I tell the model something may happen, its goal is to then assign the event an exact probability of occurring based on all of its training.

## Setup

To get this project up and running on your local machine, follow these steps:

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/RatnaAnimesh/predictive-analysis.git
    cd predictive-analysis
    ```

2.  **Create and activate a virtual environment:**
    It's highly recommended to use a virtual environment to manage project dependencies.
    ```bash
    python3 -m venv geo_venv
    source geo_venv/bin/activate
    ```

3.  **Install dependencies:**
    Install the required Python packages. You might need to create a `requirements.txt` file first if it doesn't exist.
    ```bash
    pip install pandas numpy nltk gdeltdoc tqdm lxml
    ```
    *(Note: `lxml` is for `xml.etree.ElementTree` parsing, `gdeltdoc` is for GDELT API, `tqdm` for progress bars)*

4.  **Download NLTK data:**
    The project uses NLTK's VADER lexicon for sentiment analysis.
    ```bash
    python -c "import nltk; nltk.download('vader_lexicon')"
    ```

## Usage

The project operates in two main phases: Historical Analysis (to build a baseline) and Online Anomaly Detection.

### Phase 1: Historical Analysis (Building the Baseline)

This phase processes historical GDELT data to build a statistical model of "normal" news activity.

1.  **Run the data download and processing launcher:**
    This will start multiple parallel processes to download and analyze GDELT data for each year (2015-2024 by default). This process can take a very long time and download a lot of data.
    ```bash
    python launcher.py
    ```
    *(Note: This will create `partial_results_YYYY.json` files for each year and download raw GDELT data into the `temp_data` directory, which is automatically cleaned up.)*

2.  **Combine the historical results:**
    Once all the launcher processes are complete, run the reducer to combine the yearly statistics into a single `model_state.json` file.
    ```bash
    python reducer.py
    ```

### Phase 2: Online Anomaly Detection

After building the `model_state.json` baseline, you can run the anomaly detector to monitor current news.

1.  **Run the online anomaly detector:**
    This script will download recent GDELT data, compare it against your baseline, and print alerts for significant deviations.
    ```bash
    python online_anomaly_detector.py
    ```

## Data Sources

*   **GDELT Project:** The primary source for global news event data. The project downloads GKG 2.0 data in 15-minute intervals.
*   **Newsroom Dataset:** (Mentioned in `main.py` but not actively used in the core pipeline for anomaly detection). This might be for future development or a separate analysis.

## Project Structure

```
.
├── .gitignore
├── debug_xml.py                # Utility for debugging XML parsing (likely from GDELT extras)
├── download_gdelt_data.py      # Script to download GDELT baseline data (used by historical_analyzer)
├── historical_analyzer.py      # Script for single-threaded historical analysis (superseded by mapper/reducer)
├── launcher.py                 # Orchestrates parallel 'mapper' processes for historical analysis
├── main.py                     # Example script for loading Newsroom dataset (not part of core pipeline)
├── mapper.py                   # Processes GDELT data for a specific year in parallel
├── online_anomaly_detector.py  # Detects anomalies in real-time GDELT data against a baseline
├── reducer.py                  # Combines results from parallel 'mapper' processes
├── model_state.json            # Stores the learned statistical baseline (generated by reducer)
├── partial_results_YYYY.json   # Intermediate results from 'mapper' processes
├── data/                       # Directory for processed/baseline data (e.g., gdelt_baseline, newsroom_processed)
│   └── gdelt_baseline/         # GDELT baseline data (downloaded by download_gdelt_data.py)
│   └── newsroom_processed/     # Processed Newsroom dataset (if used)
└── geo_venv/                   # Python virtual environment
```