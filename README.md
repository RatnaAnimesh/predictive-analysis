# Geopolitical Predictor

This project implements a system for geopolitical anomaly detection and aims to extend into predictive and corroboration analysis based on news data. It processes large volumes of GDELT news data to identify unusual patterns in entity mentions and sentiment.

## Table of Contents

- [Project Goals](#project-goals)
- [Setup](#setup)
- [Usage](#usage)
- [Data Sources](#data-sources)
- [Project Structure](#project-structure)

## Project Goals

1.  **Anomaly Detection:** Figure out if something is showing up in the news more to then check what's happening. Alerts if Z-score of occurrence is higher than 3.
2.  **Predictive Analysis:** Look at the latest news and try to figure out what *could* happen and assign an exact probability to the event. Should display major geopolitical or financial events occurring with probabilities more than 50%.
3.  **Corroboration Analysis:** Say I tell the model something may happen, its goal is to then assign the event an exact probability of occurring based on all of its training.

## Setup

To get this project up and running on your local machine, follow these steps:

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/RatnaAnimesh/predictive-analysis.git
    cd predictive-analysis
    ```

2.  **Create and activate a virtual environment:**
    It's highly recommended to use a virtual environment to manage project dependencies.
    ```bash
    python3 -m venv geo_venv
    source geo_venv/bin/activate
    ```

3.  **Install dependencies:**
    Install the required Python packages.
    ```bash
    pip install pandas numpy nltk gdeltdoc tqdm lxml spacy
    ```

4.  **Download NLP Models:**
    The project uses NLTK for sentiment and spaCy for entity recognition.
    ```bash
    python -c "import nltk; nltk.download('vader_lexicon')"
    python -m spacy download en_core_web_sm
    ```

## Usage

The project operates in two main phases: Historical Analysis (to build a baseline) and Online Anomaly Detection.

### Phase 1: Historical Analysis (Building the Baseline)

This phase processes historical GDELT data to build a statistical model of "normal" news activity and discover new entities.

1.  **Run the historical analyzer:**
    This single script now handles the entire process. It will download and analyze GDELT data sequentially. This process is resumable but will take a very long time.
    ```bash
    python historical_analyzer.py
    ```

### Phase 2: Online Anomaly Detection

After building the `model_state.json` baseline, you can run the anomaly detector to monitor current news.

1.  **Run the online anomaly detector:**
    This script will download recent GDELT data, compare it against your baseline, and print alerts for significant deviations.
    ```bash
    python online_anomaly_detector.py
    ```

## Data Sources

*   **GDELT Project:** The primary source for global news event data. The project downloads GKG 2.0 data in 15-minute intervals.
*   **Newsroom Dataset:** (Mentioned in `main.py` but not actively used in the core pipeline for anomaly detection). This might be for future development or a separate analysis.

## Project Structure

```
.
├── .gitignore
├── debug_xml.py                # Utility for debugging XML parsing (likely from GDELT extras)
├── download_gdelt_data.py      # Script to download GDELT baseline data (used by historical_analyzer)
├── historical_analyzer.py      # Script for single-threaded historical analysis (superseded by mapper/reducer)
├── launcher.py                 # Orchestrates parallel 'mapper' processes for historical analysis
├── main.py                     # Example script for loading Newsroom dataset (not part of core pipeline)
├── mapper.py                   # Processes GDELT data for a specific year in parallel
├── online_anomaly_detector.py  # Detects anomalies in real-time GDELT data against a baseline
├── reducer.py                  # Combines results from parallel 'mapper' processes
├── model_state.json            # Stores the learned statistical baseline (generated by reducer)
├── partial_results_YYYY.json   # Intermediate results from 'mapper' processes
├── data/                       # Directory for processed/baseline data (e.g., gdelt_baseline, newsroom_processed)
│   └── gdelt_baseline/         # GDELT baseline data (downloaded by download_gdelt_data.py)
│   └── newsroom_processed/     # Processed Newsroom dataset (if used)
└── geo_venv/                   # Python virtual environment
```